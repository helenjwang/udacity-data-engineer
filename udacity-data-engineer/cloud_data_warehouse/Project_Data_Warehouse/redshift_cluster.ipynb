{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the packages that I will need.\n",
    "import configparser\n",
    "import datetime\n",
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "#import pyarrow.parquet as pq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'AWS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-af5c2bdaf23b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AWS_ACCESS_KEY_ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AWS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AWS_ACCESS_KEY_ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AWS_SECRET_ACCESS_KEY'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AWS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AWS_SECRET_ACCESS_KEY'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/configparser.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_section\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_section\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_proxies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'AWS'"
     ]
    }
   ],
   "source": [
    "#set up the configuration and provide the AWS credential to start the project\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "\n",
    "config.sections()\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating spark session on AWS\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SparkSession' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b1421ed5afdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Creating spark session on AWS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_spark_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-b1421ed5afdf>\u001b[0m in \u001b[0;36mcreate_spark_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Create a spark session with hadoop-aws package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_spark_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.jars.packages\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"org.apache.hadoop:hadoop-aws:2.7.0\"\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SparkSession' is not defined"
     ]
    }
   ],
   "source": [
    "#CREATE SPARK SEASSION\n",
    "#Create a spark session with hadoop-aws package\n",
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "print('Creating spark session on AWS')\n",
    "spark = create_spark_session()\n",
    "\n",
    "#DATA\n",
    "input_data = \"s3a://udacity-dend/\"\n",
    "song_input_data = \"data/song-data/song_data/A/A/A/*.json\"\n",
    "log_input_data = \"data/log-data/*.json\"\n",
    "output_data = \"s3a://ashley-dend-udacity-p4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROCESS SONG DATA\n",
    "# get filepath to song data file\n",
    "print('Read song data from json file')\n",
    "song_data = spark.read.json(song_input_data)\n",
    "    \n",
    "# read song data file\n",
    "print('Print song data schema')\n",
    "df = song_data\n",
    "print(df.count())\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create songs table\n",
    "print('Extract columns to create song table')\n",
    "artist_id = \"artist_id\"\n",
    "artist_latitude = \"artist_latitude\"\n",
    "artist_location = \"artist_location\"\n",
    "artist_longitude = \"artist_longitude\"\n",
    "artist_name = \"artist_name\"\n",
    "duration = \"duration\"\n",
    "num_songs = \"num_songs\"\n",
    "song_id = \"song_id\"\n",
    "title = \"title\"\n",
    "year = \"year\"\n",
    "    \n",
    "#print('Songs table: ')\n",
    "songs_table = df.select(song_id, title, artist_id, year, duration)\n",
    "print(songs_table.limit(5).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the 'year' type so it can be used as the partition key\n",
    "df_songs_table = songs_table.toPandas()\n",
    "year_list = list(set(df_songs_table['year'].tolist()))\n",
    "type(year_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the 'arist_id' type so it can be used as the partition key\n",
    "artist_id_list = list(set(df_songs_table['artist_id'].tolist()))\n",
    "type(arist_id_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine \n",
    "df_songs_table.loc[(df_songs_table['year']==int(1982)) & (df_songs_table['artist_id']==str('AR7G5I41187FB4CE6C'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write songs table to parquet files partitioned by year and artist\n",
    "print('Writing to parquet')\n",
    "#songs_table.write.parquet(\"{}songs_table.parquet\".format(output_data))\n",
    "for year in year_list:\n",
    "    for artist_id in artist_id_list:\n",
    "        df_to_parquet = df_songs_table.loc[(df_songs_table['year']==int(year)) & (df_songs_table['artist_id']==str(artist_id))]\n",
    "        df_to_parquet.to_parquet(\"{}/songs_table/{}/{}/songs_table.parquet\".format(output_data,year,artist_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create artists table\n",
    "print('Artist table: ')\n",
    "artists_table = df.select(artist_id, artist_name, artist_location, artist_latitude, artist_longitude)\n",
    "print(artists_table.limit(5).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write artists table to parquet files\n",
    "#print('Writing to parquet')\n",
    "#artist_table.write.parquet(\"artist_table.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filepath to log data file\n",
    "log_data = spark.read.json(log_input_data)\n",
    "\n",
    "# read log data file\n",
    "print('Print song data schema')\n",
    "log_df = log_data\n",
    "print(df.count())\n",
    "log_df.printSchema()\n",
    "print(log_df.limit(5).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by actions for song plays\n",
    "# using df = \n",
    "\n",
    "# extract columns for users table  \n",
    "#print('Extract columns to create log table')\n",
    "artist= 'artist'\n",
    "auth= 'auth'\n",
    "firstName= 'firstName'\n",
    "gender= 'gender'\n",
    "itemInSession= 'itemInSession'\n",
    "lastName= 'lastName'\n",
    "length= 'length'\n",
    "level= 'level'\n",
    "location= 'location'\n",
    "method= 'method'\n",
    "page= 'page'\n",
    "registration= 'registration'\n",
    "sessionId= 'sessionId'\n",
    "song= 'song'\n",
    "status= 'status'\n",
    "ts= 'ts'\n",
    "userAgent= 'userAgent'\n",
    "userId= 'userId'\n",
    "timestamp='timestamp'\n",
    "start_time='start_time'\n",
    "hour = 'hour'\n",
    "day='day'\n",
    "week='week'\n",
    "month='month'\n",
    "year='year'\n",
    "weekday='weekday'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Users table: ')\n",
    "users_table = log_df.select(firstName, lastName, gender, level, userId)\n",
    "print(users_table.limit(5).toPandas())\n",
    "\n",
    "#write users table to parquet files\n",
    "#print('Writing to parquet')\n",
    "#users_table.write.parquet(\"users_table.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create timestamp column from original timestamp column\n",
    "get_timestamp = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000), TimestampType())\n",
    "log_df = log_df.withColumn(\"timestamp\", get_timestamp(log_df.ts))\n",
    "log_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create datetime column from original timestamp column\n",
    "get_datetime = udf(lambda x: F.to_date(x), TimestampType())\n",
    "log_df = log_df.withColumn(\"start_time\", get_timestamp(log_df.ts))\n",
    "log_df.printSchema()\n",
    "log_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create time table\n",
    "log_df = log_df.withColumn(\"hour\", F.hour(\"timestamp\"))\n",
    "log_df = log_df.withColumn(\"day\", F.dayofweek(\"timestamp\"))\n",
    "log_df = log_df.withColumn(\"week\", F.weekofyear(\"timestamp\"))\n",
    "log_df = log_df.withColumn(\"month\", F.month(\"timestamp\"))\n",
    "log_df = log_df.withColumn(\"year\", F.year(\"timestamp\"))\n",
    "log_df = log_df.withColumn(\"weekday\", F.dayofweek(\"timestamp\"))\n",
    "#log_df.printSchema()\n",
    "print(log_df.limit(5).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_table = log_df.select(start_time, hour, day, week, month, year, weekday)\n",
    "# write time table to parquet files partitioned by year and month\n",
    "#print('Writing to parquet')\n",
    "#time_table.write.parquet(\"time_table.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in song data to use for songplays table\n",
    "song_df = spark.read.json(song_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.createOrReplaceTempView(\"log_df_table\")\n",
    "song_df.createOrReplaceTempView(\"song_df_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns from joined song and log datasets to create songplays table \n",
    "# songplay_id, \n",
    "# start_time, \n",
    "# user_id, \n",
    "# level, \n",
    "# song_id, \n",
    "# artist_id, \n",
    "# session_id, \n",
    "# location, \n",
    "# user_agent\n",
    "\n",
    "songplays_table = spark.sql(\n",
    "    \"\"\"SELECT log_df_table.start_time, log_df_table.userId, log_df_table.level, log_df_table.sessionId, log_df_table.location, log_df_table.userAgent, song_df_table.song_id, song_df_table.artist_id \n",
    "    FROM log_df_table \n",
    "    INNER JOIN song_df_table \n",
    "    ON song_df_table.artist_name = log_df_table.artist \n",
    "    \"\"\")\n",
    "#ON song_df_table.title = log_df_table.song\n",
    "#ON song_df_table.artist_name = log_df_table.artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_table = songplays_table.withColumn(\"songplay_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the data to make sure it is valid\n",
    "\n",
    "songplays_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write songplays table to parquet files partitioned by year and month\n",
    "print('Writing to parquet')\n",
    "songplays_table.write.parquet(\"songplays_table.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#below is for my personal use\n",
    "\n",
    "#Unzip folder for jupyter lab try\n",
    "# import zipfile\n",
    "# path_to_zip_file = 'data/log-data.zip'\n",
    "# directory_to_extract_to = 'data/log-data'\n",
    "# zip_ref = zipfile.ZipFile(path_to_zip_file, 'r')\n",
    "# zip_ref.extractall(directory_to_extract_to)\n",
    "# zip_ref.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
