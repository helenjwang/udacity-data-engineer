{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "Dataset used: the immigration dataset and city demographics dataset.\n",
    "Database used: Postgres database\n",
    "Goal: use the star schema to answer questions about where (i.e., what countries) immigrants were coming from and where in the U.S. they were headed to.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "FATAL:  password authentication failed for user \"capstone_user\"\nFATAL:  password authentication failed for user \"capstone_user\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/home/workspace/load_immigration_data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# Create database connections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONN_STRING\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# needed for DataFrame.to_sql\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsycopg2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONN_STRING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautocommit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mcur\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/psycopg2/__init__.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mdsn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dsn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection_factory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconnection_factory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwasync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcursor_factory\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: FATAL:  password authentication failed for user \"capstone_user\"\nFATAL:  password authentication failed for user \"capstone_user\"\n"
     ]
    }
   ],
   "source": [
    "%run -i 'constants.py'\n",
    "%run -i 'load_immigration_data.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a pre-data quality check: the immigration data has 28 columns\n",
    "pd.set_option('display.max_columns', 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Data I used: the immigration dataset and city demographics datasets provided by Udacity.\n",
    "Methodology: used pandas to read the data and load it into Postgres\n",
    "    \n",
    "#### Describe and Gather Data \n",
    "1. split the immigration dataset up into a single fact_immmigration table as well as several dim_ dimension tables\n",
    "2. aggregate the city demographics dataset resulted in one intial dimension table to be state-level statistics in another dimention table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data here: call the constants.py\n",
    "\n",
    "READ_CHUNK_SIZE = 300000\n",
    "IMMIGRATION_DATA_FILENAMES = [\n",
    "    'data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat',\n",
    "    'data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat',\n",
    "    'data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat',\n",
    "    'data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat',\n",
    "    'data/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat',\n",
    "    'data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat',\n",
    "    'data/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat',\n",
    "    'data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat',\n",
    "    'data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat',\n",
    "    'data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat',\n",
    "    'data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat',\n",
    "    'data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat',\n",
    "]\n",
    "CONN_STRING = 'postgresql://capstone_user:capstone_pw@localhost:5432/capstone'\n",
    "HEADER_FILE = 'data/I94_SAS_Labels_Descriptions.SAS'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/home/workspace/constants.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIMMIGRATION_DATA_FILENAMES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m iterator = pd.read_sas(\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sas7bdat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ISO-8859-1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAD_CHUNK_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[0mimmigration_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/sas/sasreader.py\u001b[0m in \u001b[0;36mread_sas\u001b[0;34m(filepath_or_buffer, format, index, encoding, chunksize, iterator)\u001b[0m\n\u001b[1;32m     59\u001b[0m         reader = SAS7BDATReader(filepath_or_buffer, index=index,\n\u001b[1;32m     60\u001b[0m                                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                 chunksize=chunksize)\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unknown SAS format'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/sas/sas7bdat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buf, index, convert_dates, blank_missing, chunksize, encoding, convert_text, convert_header_text)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_filepath_or_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path_or_buf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path_or_buf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat'"
     ]
    }
   ],
   "source": [
    "#data size is huge; just load one chunk of the data for now: \n",
    "filename = IMMIGRATION_DATA_FILENAMES[0]\n",
    "iterator = pd.read_sas(\n",
    "    filename, 'sas7bdat', encoding='ISO-8859-1', chunksize=READ_CHUNK_SIZE\n",
    ")\n",
    "immigration_df = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'immigration_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/home/workspace/constants.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimmigration_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimmigration_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'immigration_df' is not defined"
     ]
    }
   ],
   "source": [
    "print(immigration_df.shape)\n",
    "immigration_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in and print out the city demographics data\n",
    "city_demo_df = pd.read_csv('data/us-cities-demographics.csv', delimiter=';')\n",
    "print(city_demo_df.shape)\n",
    "city_demo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Step 2: Explore and Assess the Data\n",
    "\n",
    "####Data Exploration:\n",
    "Immigration Data\n",
    "Data is too big to load into a single dataframe.\n",
    "Details steps are in the below data quality check.\n",
    "\n",
    "#### Data Cleansing:\n",
    "1. most of the cleaning of this data involved creating dim tables for many of the coded fields, like i94res i94cit, and i94visa. \n",
    "2. Any data in those columns that didn't match a key in those dim tables would be interpreted as \"unknown.\"\n",
    "3. Two columns were cleaned. The first is i94bir, which is the age of the respondent in years. Anything in this table is set to be less than zero and the two 1812 values to NULL. biryear is the reported birth year of entrants and most of its dates range from 1900 - 2016, but there are two outliers of 204, two of 2018, and one from 2019. Since this data is from 2016 those cases will all get set to NULL.\n",
    "4. i94addr where the values are not clean as they still have some valuable informations \n",
    "\n",
    "Sample queries:\n",
    "\n",
    "select i94addr, count(i94addr) from fact_immigration\n",
    "where i94addr not in (select code from dim_address)\n",
    "group by i94addr order by count desc;\n",
    "gives\n",
    "\n",
    "code\tcount\n",
    "MP\t130555\n",
    "US\t108767\n",
    "VQ\t49465\n",
    "UN\t20383\n",
    "GQ\t10998\n",
    "\n",
    "##the columns in the data are consistent across each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring immigration data columns across all the files\n",
    "\n",
    "# Put a small dataframe from each month into a list\n",
    "dfs = []\n",
    "for fname in IMMIGRATION_DATA_FILENAMES:\n",
    "    myiter = pd.read_sas(fname, 'sas7bdat', encoding='ISO-8859-1', chunksize=20)\n",
    "    dfs.append(next(myiter))\n",
    "    \n",
    "# create a dict that maps each month to a list of column names\n",
    "cnames_by_fname = {t[0].split('/')[-1].split('_')[1][:3]: list(t[1].columns.values)\n",
    "                   for t in zip(IMMIGRATION_DATA_FILENAMES, dfs)}\n",
    "\n",
    "# look for some commonality and reverse that dict so a hashed up comma-seperated\n",
    "# list of the column names is the key and the values are lists of the months\n",
    "cbyf_reversed = defaultdict(list)\n",
    "for k, v in cnames_by_fname.items():\n",
    "    cbyf_reversed[','.join(v)].append(k)\n",
    "    \n",
    "print(len(cbyf_reversed))\n",
    "cbyf_reversed.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I saw that June is the odd duck out; so I took a closer look at how these columns\n",
    "# do and don't match up then\n",
    "\n",
    "usual = list(dfs[0].columns.values)\n",
    "gray_duck = list(dfs[5].columns.values)\n",
    "print(len(usual), len(gray_duck))\n",
    "for z in (zip(usual, gray_duck)):\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# June just has a `validres` and then five `delete_` columns added\n",
    "# to the middle. validation below:\n",
    "\n",
    "usual == [gd for gd in gray_duck if not (gd.startswith('delete_') or gd == 'validres')]\n",
    "\n",
    "#have to take special care with the June data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring city demographics data\n",
    "\n",
    "# Looking at one city reveals grain to be city/state/race\n",
    "city_demo_df[city_demo_df['City'] == 'Silver Spring'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More city demographics exploration\n",
    "\n",
    "# Ensure counts, when present, don't exceed total population, and that male+female == total\n",
    "for _, row in city_demo_df.iterrows():\n",
    "    if pd.notnull(row['Male Population']):\n",
    "        assert row['Male Population'] + row['Female Population'] == row['Total Population']\n",
    "    if pd.notnull(row['Number of Veterans']):\n",
    "        assert row['Number of Veterans'] <= row['Total Population']\n",
    "    if pd.notnull(row['Foreign-born']):\n",
    "        assert row['Foreign-born'] <= row['Total Population']\n",
    "    if pd.notnull(row['Count']):\n",
    "        assert row['Count'] <= row['Total Population']\n",
    "    for numbered_column in ['Male Population', 'Female Population', 'Total Population', 'Number of Veterans', 'Foreign-born', 'Count']:\n",
    "        assert pd.isnull(row[numbered_column]) or row[numbered_column] > 0\n",
    "        \n",
    "# Check to see if race counts add up to total\n",
    "sub_df = city_demo_df[['City', 'State', 'Total Population', 'Count']]\n",
    "grouped = sub_df.groupby(['City', 'State', 'Total Population']).sum().reset_index()\n",
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More city demographics exploration\n",
    "\n",
    "# From the last run, it looks like Count always sums up to >= Total Population; validate below:\n",
    "for _, row in grouped.iterrows():\n",
    "    if pd.notnull(row['Total Population']) and row['Count'] < row['Total Population']:\n",
    "        print(row['City'], row['State'], row['Total Population'], row['Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "1. to know more about immigration events, I put the immigration data at the center of my star schema in a table called fact_immigration. \n",
    "\n",
    "Sample SQL below:\n",
    "    create table fact_immigration\n",
    "(\n",
    "    immigration_id serial not null\n",
    "        constraint fact_immigration_pkey\n",
    "            primary key,\n",
    "    cicid integer not null,\n",
    "    i94yr integer not null,\n",
    "    i94mon integer not null,\n",
    "    i94cit integer,\n",
    "    i94res integer,\n",
    "    i94port char(3),\n",
    "    arrdate integer,\n",
    "    i94mode integer,\n",
    "    i94addr char(3),\n",
    "    depdate integer,\n",
    "    i94bir integer,\n",
    "    i94visa integer,\n",
    "    count integer,\n",
    "    dtadfile varchar,\n",
    "    visapost char(3),\n",
    "    occup char(3),\n",
    "    entdepa char,\n",
    "    entdepd char,\n",
    "    entdepu char,\n",
    "    matflag char,\n",
    "    biryear integer,\n",
    "    dtaddto integer,\n",
    "    gender char,\n",
    "    insnum integer,\n",
    "    airline char(2),\n",
    "    admnum integer,\n",
    "    fltno varchar,\n",
    "    visatype char(2)\n",
    ");\n",
    "\n",
    "I was able to create several dimension tables around it:\n",
    "\n",
    "dim_country\n",
    "dim_arrival_mode\n",
    "dim_port\n",
    "dim_address\n",
    "dim_visa_type\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "1. dim_arrival_mode and dim_visa_type:creating simple INSERT statements.\n",
    "2. dim_address, dim_port, and dim_country:\n",
    "2.1 Inspecting the line numbers where their details are listed in the header file\n",
    "2.2 Crafting regular expressions\n",
    "2.3 Writing a script to parse values from those lines and insert the values\n",
    "3. dim_city and dim_state: ready to be inserted into the database\n",
    "4. fact_immigration: write the 'load_immigration_data.py' to solve the problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database connections\n",
    "conn = psycopg2.connect(CONN_STRING)\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dim_arrival_mode and dim_visa_type\n",
    "dim_arr_mode_drop = 'DROP TABLE IF EXISTS dim_arrival_mode;'\n",
    "\n",
    "dim_arr_mode_create = \"\"\"CREATE TABLE IF NOT EXISTS dim_arrival_mode\n",
    "(code int, mode char(12))\"\"\"\n",
    "\n",
    "dim_arr_mode_insert = \"\"\"INSERT INTO dim_arrival_mode (code, mode)\n",
    "VALUES (1, 'Air'), (2, 'Sea'), (3, 'Land'), (9, 'Not reported');\"\"\"\n",
    "\n",
    "cur.execute(dim_arr_mode_drop)\n",
    "cur.execute(dim_arr_mode_create)\n",
    "cur.execute(dim_arr_mode_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_visa_type_drop = 'DROP TABLE IF EXISTS dim_visa_type;'\n",
    "\n",
    "dim_visa_type_create = \"\"\"CREATE TABLE IF NOT EXISTS dim_visa_type\n",
    "(code int, visa_type char(8))\"\"\"\n",
    "\n",
    "dim_visa_type_insert = \"\"\"INSERT INTO dim_visa_type (code, visa_type)\n",
    "VALUES (1, 'Business'), (2, 'Pleasure'), (3, 'Student');\"\"\"\n",
    "\n",
    "cur.execute(dim_visa_type_drop)\n",
    "cur.execute(dim_visa_type_create)\n",
    "cur.execute(dim_visa_type_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse header file and display nicely before going straight into dim_address\n",
    "with open(HEADER_FILE) as f:\n",
    "    header_file_lines = f.readlines()\n",
    "    \n",
    "comment_lines = [line for line in header_file_lines if line.startswith('/*') and line.endswith('*/\\n')]\n",
    "clpatt = re.compile(r'^/\\*\\s+(?P<code>.+?)\\s+-\\s+(?P<description>.+)\\s+\\*/$')\n",
    "matches = [clpatt.match(cl) for cl in comment_lines]\n",
    "if not all(m is not None for m in matches):\n",
    "    for i, m in enumerate(matches):\n",
    "        if m is None:\n",
    "            print(i)\n",
    "print(f'CODE{\"\":16}', 'DESCRIPTION')\n",
    "for m in matches:\n",
    "    print(f'{m.group(\"code\"):20}', m.group('description'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse header file for dim_adddress values\n",
    "address_lines = header_file_lines[981:1036]\n",
    "patt = re.compile(r\"^\\s*'(?P<code>..)'\\s*=\\s*'(?P<name>.+)'.*$\")\n",
    "matches = [patt.match(line) for line in address_lines]\n",
    "address_codes = {match.group('code'): match.group('name') for match in matches}\n",
    "assert len(address_codes) == len(address_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up queries\n",
    "dim_address_drop = 'DROP TABLE IF EXISTS dim_address;'\n",
    "dim_address_create = 'CREATE TABLE IF NOT EXISTS dim_address (code char(2), name varchar);'\n",
    "dim_address_insert = 'INSERT INTO dim_address (code, name) VALUES (%s, %s);'\n",
    "\n",
    "# execute queries\n",
    "cur.execute(dim_address_drop)\n",
    "cur.execute(dim_address_create)\n",
    "for item in sorted(address_codes.items()):\n",
    "    cur.execute(dim_address_insert, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dim port\n",
    "# parse header file for dim_adddress values\n",
    "port_lines = header_file_lines[302:962]\n",
    "patt = re.compile(r\"^\\s*'(?P<code>...?)'\\s*=\\s*'(?P<name>.+)'.*$\")\n",
    "matches = [patt.match(line) for line in port_lines]\n",
    "port_codes = {match.group('code'): match.group('name').strip() for match in matches}\n",
    "assert len(port_codes) == len(port_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up queries\n",
    "dim_port_drop = 'DROP TABLE IF EXISTS dim_port;'\n",
    "dim_port_create = 'CREATE TABLE IF NOT EXISTS dim_port (code char(3), name varchar);'\n",
    "dim_port_insert = 'INSERT INTO dim_port (code, name) VALUES (%s, %s);'\n",
    "\n",
    "# execute queries\n",
    "cur.execute(dim_port_drop)\n",
    "cur.execute(dim_port_create)\n",
    "for item in sorted(port_codes.items()):\n",
    "    cur.execute(dim_port_insert, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dim country\n",
    "# parse header file for dim_country values\n",
    "country_lines = header_file_lines[9:298]\n",
    "patt = re.compile(r\"^\\s*(?P<code>\\d+)\\s*=\\s*'(?P<country>.+)'.*$\")\n",
    "matches = [patt.match(line) for line in country_lines]\n",
    "country_codes = {int(match.group('code')): match.group('country') for match in matches}\n",
    "assert len(country_lines) == len(country_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up queries\n",
    "dim_country_drop = 'DROP TABLE IF EXISTS dim_country;'\n",
    "\n",
    "dim_country_create = \"\"\"CREATE TABLE IF NOT EXISTS dim_country\n",
    "(code int PRIMARY KEY, name varchar NOT NULL);\"\"\"\n",
    "\n",
    "dim_country_insert = \"\"\"INSERT INTO dim_country\n",
    "(code, name)\n",
    "VALUES (%s, %s)\n",
    "ON CONFLICT (code) DO NOTHING;\"\"\"\n",
    "\n",
    "# execute queries\n",
    "cur.execute(dim_country_drop)\n",
    "cur.execute(dim_country_create)\n",
    "for item in country_codes.items():\n",
    "    cur.execute(dim_country_insert, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dim date\n",
    "#there are so many different ways in the immigration data. I did a transformation to unify to YY-MM-DD for better understanding\n",
    "# set up queries\n",
    "dim_date_drop = 'DROP TABLE IF EXISTS dim_date;'\n",
    "\n",
    "dim_date_create = \"\"\"CREATE TABLE dim_date\n",
    "(code int PRIMARY KEY, year int NOT NULL, month int NOT NULL,\n",
    " day int NOT NULL, day_of_week INT NOT NULL, ymd_dash char(10) NOT NULL,\n",
    " ymd_nodash char(8) NOT NULL, mdy_nodash char(8) NOT NULL);\n",
    "\"\"\"\n",
    "\n",
    "dim_date_insert = \"\"\"INSERT INTO dim_date\n",
    "(code, year, month, day, day_of_week, ymd_dash, ymd_nodash, mdy_nodash)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s);\"\"\"\n",
    "\n",
    "# execute queries\n",
    "cur.execute(dim_date_drop)\n",
    "cur.execute(dim_date_create)\n",
    "\n",
    "dt = datetime(2016, 1, 1)\n",
    "end_dt = datetime(2019, 12, 31)\n",
    "one_day = timedelta(days=1)\n",
    "code = 20454\n",
    "\n",
    "while dt <= end_dt:\n",
    "    cur.execute(dim_date_insert, \n",
    "               [code, dt.year, dt.month, dt.day, dt.weekday(), dt.strftime('%Y-%m-%d'),\n",
    "                dt.strftime('%Y%m%d'), dt.strftime('%d%m%Y')]\n",
    "               )\n",
    "    dt = dt + one_day\n",
    "    code += 1\n",
    "    \n",
    "dt = datetime(2015, 12, 31)\n",
    "end_dt = datetime(1900, 1, 1)\n",
    "code = 20453\n",
    "\n",
    "while dt >= end_dt:\n",
    "    cur.execute(dim_date_insert, \n",
    "               [code, dt.year, dt.month, dt.day, dt.weekday(), dt.strftime('%Y-%m-%d'),\n",
    "                dt.strftime('%Y%m%d'), dt.strftime('%d%m%Y')]\n",
    "               )\n",
    "    dt = dt - one_day\n",
    "    code -=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dim city\n",
    "# set up queries\n",
    "dim_city_drop = 'DROP TABLE IF EXISTS dim_city;'\n",
    "\n",
    "dim_city_create = \"\"\"CREATE TABLE IF NOT EXISTS dim_city\n",
    "(city varchar, state varchar, median_age numeric, male_pop int, female_pop int, total_pop int, num_vets int,\n",
    "foreign_born int, avg_household_size float, state_code char(2), race varchar, count int);\n",
    "\"\"\"\n",
    "\n",
    "dim_city_insert = \"\"\"INSERT INTO dim_city\n",
    "(city, state, median_age, male_pop, female_pop, total_pop, num_vets, foreign_born, avg_household_size, state_code,\n",
    "race, count)\n",
    "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);\"\"\"\n",
    "\n",
    "# execute queries\n",
    "cur.execute(dim_city_drop)\n",
    "cur.execute(dim_city_create)\n",
    "for _, row in city_demo_df.iterrows():\n",
    "    cur.execute(dim_city_insert, [v if pd.notna(v) else None for v in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set up queries\n",
    "dim_state_drop = 'DROP TABLE IF EXISTS dim_state;'\n",
    "\n",
    "dim_state_create = \"\"\"CREATE TABLE IF NOT EXISTS dim_state\n",
    "(state_code char(2) PRIMARY KEY, male_pop int, female_pop int, total_pop int, foreign_born int);\"\"\"\n",
    "\n",
    "dim_state_insert = \"\"\"INSERT INTO dim_state\n",
    "(state_code, male_pop, female_pop, total_pop, foreign_born)\n",
    "VALUES (%s, %s, %s, %s, %s);\"\"\"\n",
    "\n",
    "# execute queries\n",
    "cur.execute(dim_state_drop)\n",
    "cur.execute(dim_state_create)\n",
    "for _, row in summed_by_state_df.iterrows():\n",
    "    cur.execute(dim_state_insert, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##4.2 Data Quality Check:\n",
    "\n",
    "###row counts for fact_immigration\n",
    "I logged the following row counts when running load_immigration_data.py\n",
    "\n",
    "month\tdf rows\n",
    "jan\t2,847,924\n",
    "feb\t2,570,543\n",
    "mar\t3,157,072\n",
    "apr\t3,096,313\n",
    "may\t3,444,249\n",
    "jun\t3,574,989\n",
    "jul\t4,265,031\n",
    "aug\t4,103,570\n",
    "sep\t3,733,786\n",
    "oct\t3,649,136\n",
    "nov\t2,914,926\n",
    "dec\t3,432,990\n",
    "This allowed me to run the following query and compare the results\n",
    "\n",
    "select count(distinct immigration_id), i94mon\n",
    "from fact_immigration\n",
    "group by i94mon\n",
    "order by i94mon;\n",
    "Only 2016 data\n",
    "I also assured myself there was no data outside of 2016\n",
    "\n",
    "select count(1) from fact_immigration where i94yr is null or i94yr != 2016;\n",
    "\n",
    "count\n",
    "I make sure the count field is always 1 with\n",
    "\n",
    "select count(count) N, count\n",
    "from fact_immigration\n",
    "group by count\n",
    "order by N desc;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###4.3 Data Dictionary\n",
    "\n",
    "The grain is an immigration event\n",
    "\n",
    "immigration_id: primary key\n",
    "cicid: unique key within a month\n",
    "i94yr: 4 digit year, always 2016\n",
    "i94mon: numeric month, 1-12\n",
    "i94cit: immigrant's country of citizenship; foreign key to dim_country\n",
    "i94res: immigrant's country of residence outside US; foreign key to dim_country\n",
    "i94port: port of entry; foreign key to dim_port\n",
    "arrdate: arrival date of immigrant where 20454 == 1/1/2016\n",
    "i94mode: mode of arrival; foreign key to dim_arrival_mode\n",
    "i94addr: address (usually state) of immigrant in US; foreign key to dim_address\n",
    "depdate: departure date of immigrant where 20454 == 1/1/2016\n",
    "i94bir: immigrant's age in years\n",
    "i94visa: foreign key to dim_visa_type\n",
    "count: used for summary statistics; always 1 (for easy adding)\n",
    "dtadfile: dates in the format YYYYMMDD\n",
    "visapost: three-letter codes corresponding to where visa was issued\n",
    "occup: occupation in US of immigration. Mostly STU for student, also many OTH for other\n",
    "entdepa: one-letter arrival code\n",
    "entdepd: one-letter departure code\n",
    "entdepu: one-letter update code\n",
    "matflag: M if the arrival and departure records match\n",
    "biryear: four-digit year of birth\n",
    "dtaddto: MMDDYYYY date field for when the immigrant is admitted until\n",
    "gender: mostly M and F, but some X and U as well\n",
    "insnum: Immigration and Naturalization Services number; many re-used\n",
    "airline: Airline of entry for immigrant\n",
    "admnum: admission number; many re-used, but not as much as insnum\n",
    "fltno: flight number of immigrant\n",
    "visatype: short visa codes like WT, B2, WB, etc.\n",
    "dim_city\n",
    "Provides population statistics on cities in the US. Grain is city/state/race.\n",
    "\n",
    "city: city's name\n",
    "state: state city is in\n",
    "median_age: median age of city\n",
    "male_pop: number of men in the city\n",
    "female_pop: number of women in the city\n",
    "total_pop: number of people in the city\n",
    "num_vets: number of veterans in the city\n",
    "foreign_born: number of foreign-born people in the city\n",
    "avg_household_size: average household size\n",
    "state_code: two-letter code for state\n",
    "race: White, Hispanic or Latino, Asian, Black or African-American, or American Indian and Alaska Native\n",
    "count: number of people of that race in the city\n",
    "dim_state\n",
    "Aggregated statistics from dim_city by state\n",
    "\n",
    "state_code: two-letter code for state\n",
    "male_pop: number of men in the state\n",
    "female_pop: number of women in the state\n",
    "total_pop: number of people in the state\n",
    "foreign_born: number of foreign-born people in the state\n",
    "dim_country\n",
    "A list of countries and their codes that appear in fact_immigration.i94cit and fact_immigration.i94res\n",
    "\n",
    "code: a numbered code\n",
    "name: usually a name of a country. There are many that start with INVALID: as well as several different No Country Code([code]) values\n",
    "dim_address\n",
    "A list of the states (usually) where immigrants list their address\n",
    "\n",
    "code: mostly two-letter codes for states. There's DC, GU (Guam), and 99 (All Other Codes) as well\n",
    "name: name of state, region, etc.\n",
    "dim_port\n",
    "A list of the ports of arrival\n",
    "\n",
    "code: a short code\n",
    "name: the name of the port; there are some No PORT Code ([code]) values too\n",
    "dim_date\n",
    "A list of dates in different formats\n",
    "\n",
    "code: the CIC code for date where 20454 is 1/1/2016\n",
    "year: four-digit year\n",
    "month: month; 1-12\n",
    "day: day; 1-31\n",
    "day_of_week: 0 for Monday, 1 for Tuesday, ..., 7 for Sunday\n",
    "ymd_dash: date formatted as YYYY-MM-DD\n",
    "ymd_nodash: date formatted as YYYYMMDD\n",
    "mdy_noash: date formatted as MMDDYYYY\n",
    "dim_arrival_mode\n",
    "How immigrants arrived. Foreign key to fact_immigration.i94mode\n",
    "\n",
    "code: 1, 2, 3, or 9\n",
    "mode: Air, Sea, Land, or Not reported, respectively\n",
    "dim_visa_type\n",
    "The type of visa the immigrant is coming in on. Foreigy key to fact_immigration.i94visa\n",
    "\n",
    "code: 1, 2, or 3\n",
    "visa_type: Business, Pleasure, or Student, respectively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "## tools and technologies for the project.\n",
    "Python and Pandas can easily read all the data formats provided and then easily get them into a relational database.\n",
    "\n",
    "The data was structured and formatted well enough to make using a SQL relational database a good fit;\n",
    "It is also an easily queryable star schema;\n",
    "Postgres is fast and robust.\n",
    "\n",
    "## how often the data should be updated and the rationale\n",
    "1. fact_immigration needs to be updated monthly when each new dataset is available\n",
    "2. with a header file, all dim_ tables source from the immigration data can be dropped and recreated completely as above\n",
    "3. dim_city and dim_state don't have a time component now. If new data is available in the future they should be updated at that time, ideally with date columns, at least by year\n",
    "4. dim_date should be kept up to date. To be safe all dates between through 12/31/2099 should be added\n",
    "\n",
    "## how I would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "---convert the fact_immigration data to a format readable by Redshift Spectrum and land the data to S3, partitioned by date, and create an external schema so Redshift Spectrum could read it in a schema-on-read fashion.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "---build an Airflow DAG using a S3Sensor that kicked off upon its arrival and then proceeded to parse the data, land it in its date-partitioned location in S3, in which case it would be ready for Redshift Spectrum to read immediately. \n",
    " * The database needed to be accessed by 100+ people.\n",
    "---have the data replicate to different nodes used by different users. If the users are located around the world, a replication node near each group of people would be best."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
